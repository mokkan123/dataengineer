Common values:

vers=1.0 â†’ SMB1 (old, insecure, but still needed for legacy systems).

vers=2.0 â†’ SMB2.0 (better performance & security than SMB1).

vers=2.1 â†’ SMB2.1 (Windows 7 / Server 2008 R2).

vers=3.0 â†’ SMB3.0 (Windows 8 / Server 2012; adds encryption, better durability).

vers=3.02 â†’ SMB3.02 (Windows 8.1 / Server 2012 R2).

vers=3.1.1 â†’ SMB3.1.1 (Windows 10 / Server 2016; adds pre-auth integrity, improved security).

ðŸ‘‰ If you donâ€™t specify vers=, the kernel tries to auto-negotiate. But for many servers (e.g., Azure Files), you must explicitly specify vers=3.0 or later.

2. sec (Security mode / authentication mechanism)

Purpose: Defines the authentication protocol used between client and server.

Why it matters:
Not all servers support all security modes. Using the wrong one causes Permission denied or mount error(13) failures.

Common values:

sec=ntlm â†’ Uses NTLM (old, weak).

sec=ntlmv2 â†’ Uses NTLMv2 (stronger, recommended).

sec=ntlmssp â†’ NTLMv2 encapsulated in SPNEGO (used by many modern Windows servers).

sec=krb5 â†’ Kerberos authentication (requires Kerberos setup).

sec=krb5i â†’ Kerberos with integrity checking.

sec=krb5p â†’ Kerberos with integrity + encryption.

ðŸ‘‰ By default, mount.cifs usually tries ntlmssp. If your server only supports NTLM or Kerberos, you must override it.

ðŸ”¹ Example command with both options
sudo mount.cifs //server.example.com/share /mnt/share \
  -o username=myuser,domain=MYDOMAIN,vers=3.0,sec=ntlmssp


vers=3.0 â†’ Force SMB3.0

sec=ntlmssp â†’ Use modern NTLMv2 authentication wrapped in SPNEGO


sudo mount -t cifs //nas-server/share /mnt/nas \
  -o username=<your_user>,password=<your_pass>,vers=3.0,iocharset=utf8


sudo mount.cifs //ananthan.ca/data/dremiotest123 /tmp/test123 \
   -o username=<user>,password=<password>,domain=ANANTHAN,vers=3.0,sec=ntlmssp,iocharset=utf8

Absolute Used Capacity = 8%
â†’ This queue is currently using 8% of the whole cluster (its dominant resource share across memory/vcores).

Configured Capacity = 10%
â†’ This queueâ€™s guaranteed share (aka capacity) is 10% of its parent queue. If the parent is root (100%), then its Absolute Capacity is 10% of the whole cluster.

Absolute Configured Max Capacity = 50%
â†’ This queue can grow up to 50% of the whole cluster when others are idle (soft limit). It cannot be scheduled beyond this.



(
echo "To: recipient@example.com"
echo "Subject: Test Email via sendmail"
echo
echo "This is a test email sent using sendmail from the CLI."
) | sendmail -t

sendmail -v recipient@example.com <<EOF
Subject: Test Email from Sendmail Verbose
To: recipient@example.com
From: your_email@example.com

This is the body of the test email.
EOF


(echo "To: recipient@example.com"; echo "From: your_email@example.com"; echo "Subject: Verbose Test Email"; echo ""; echo "Hello, this is a test email.") | sendmail -v -t


from pyspark.sql import SparkSession
import random

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Generate10GBCSV") \
    .config("spark.sql.shuffle.partitions", "400") \
    .config("spark.executor.instances", "1") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "12") \
    .getOrCreate()

# Function to generate random rows
def generate_row():
    return {
        "id": random.randint(1, 1_000_000),
        "name": f"name_{random.randint(1, 1_000)}",
        "value": random.uniform(1.0, 100.0),
        "category": random.choice(["A", "B", "C", "D"])
    }

# Approximate size of each row in bytes (~100 bytes)
row_size = 100

# Number of rows to generate for ~10GB
num_rows = (10 * 1024**3) // row_size  # 10GB / size per row

# Create RDD and generate DataFrame
data_rdd = spark.sparkContext.parallelize(range(int(num_rows))).map(lambda _: generate_row())
data_df = spark.createDataFrame(data_rdd)

# Output path
output_path = "/opt/data/10gb_data.csv"  # Replace with your desired output path

# Write data to CSV
data_df.write.mode("overwrite").option("header", "true").csv(output_path)

print(f"10GB CSV data written to {output_path}")

# Stop the SparkSession
spark.stop()
