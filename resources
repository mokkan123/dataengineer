https://medium.com/@harsh11csb/mastering-spark-jobs-demystifying-task-division-executor-utilization-and-instance-selection-a2a443cc797b

https://linuxconfig.org/how-to-allow-port-through-firewall-on-almalinux




Processing a 4 TB dataset on a 5-node Spark cluster with 800 cores and 3 TB of memory requires careful configuration to optimize performance. Implementing dynamic executor allocation and fine-tuning executor settings are crucial steps in this process.

Cluster Resources Overview

Total Nodes: 5
Total Cores: 800
Total Memory: 3 TB (approximately 614.4 GB per node)
Dynamic Executor Allocation

Dynamic allocation allows Spark to adjust the number of executors during runtime based on workload demands, enhancing resource utilization and efficiency. 
APACHE SPARK

To enable dynamic allocation, configure the following parameters in your spark-defaults.conf file:

properties
Copy code
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=10
spark.dynamicAllocation.maxExecutors=100
spark.dynamicAllocation.initialExecutors=20
spark.dynamicAllocation.enabled: Activates dynamic allocation.
spark.dynamicAllocation.minExecutors: Sets the minimum number of executors.
spark.dynamicAllocation.maxExecutors: Defines the maximum number of executors.
spark.dynamicAllocation.initialExecutors: Specifies the number of executors to start with.
Executor Configuration

Optimizing executor settings involves balancing the number of executors, cores per executor, and memory allocation:

Cores per Executor: Allocating 5 cores per executor is a balanced approach, promoting parallelism while preventing excessive task contention. 
SPARK BY EXAMPLES

Executors per Node: With 160 cores per node (800 cores / 5 nodes), assigning 5 cores per executor results in 32 executors per node (160 cores / 5 cores per executor).

Memory per Executor: Distributing the node's memory among the executors, while reserving resources for system processes and overhead, is essential.

Available Memory per Node: 614.4 GB
Memory Allocation: Assuming 90% allocation to Spark: 614.4 GB * 0.9 ≈ 553 GB
Memory per Executor: 553 GB / 32 executors ≈ 17.28 GB
Considering overhead (approximately 7%):

Overhead per Executor: 17.28 GB * 0.07 ≈ 1.21 GB
Executor Memory Setting: 17.28 GB - 1.21 GB ≈ 16 GB
Spark Configuration Parameters

Set the following parameters to reflect the above calculations:

properties
Copy code
spark.executor.cores=5
spark.executor.memory=16g
spark.executor.memoryOverhead=1.2g
Data Partitioning

Efficient data partitioning enhances parallelism and performance. A common guideline is to have approximately 2–4 partitions per core. 
SPARK BY EXAMPLES

Total Cores: 800
Target Partitions: 800 cores * 3 partitions per core = 2,400 partitions
Adjust the number of partitions in your Spark job accordingly:

python
Copy code
rdd.repartition(2400)
Flowchart for Optimizing Spark Job Performance

To visualize the optimization process, consider the following flowchart:

mermaid
Copy code
graph TD
A[Start] --> B[Enable Dynamic Allocation]
B --> C[Configure Executor Settings]
C --> D[Set spark.executor.cores to 5]
D --> E[Calculate Memory per Executor]
E --> F[Set spark.executor.memory to 16g]
F --> G[Set spark.executor.memoryOverhead to 1.2g]
G --> H[Determine Optimal Number of Partitions]
H --> I[Set partitions to 2400]
I --> J[Run Spark Job]
J --> K[Monitor Performance]
K --> L{Performance Satisfactory?}
L --> |Yes| M[End]
L --> |No| N[Adjust Configurations and Re-run]
N --> J
Additional Optimization Tips

Data Serialization: Switch to Kryo serialization for improved performance. 
REINTECH

properties
Copy code
spark.serializer=org.apache.spark.serializer.KryoSerializer
Broadcast Joins: For small datasets, use broadcast joins to reduce shuffle operations.

python
Copy code
broadcasted_var = sc.broadcast(small_dataset)
Caching: Cache intermediate datasets that are reused to save computation time.

python
Copy code
df.cache()
By implementing these configurations and best practices, you can enhance the performance of your Spark job on the specified cluster, ensuring efficient processing of the 4 TB dataset.












Processing a 4 TB dataset on a 5-node Spark cluster with 800 cores and 3 TB of memory requires careful configuration to optimize performance. Implementing dynamic executor allocation and fine-tuning executor settings are crucial steps in this process.

Cluster Resources Overview

Total Nodes: 5
Total Cores: 800
Total Memory: 3 TB (approximately 614.4 GB per node)
Dynamic Executor Allocation

Dynamic allocation allows Spark to adjust the number of executors during runtime based on workload demands, enhancing resource utilization and efficiency. 
APACHE SPARK

To enable dynamic allocation, configure the following parameters in your spark-defaults.conf file:

properties
Copy code
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=10
spark.dynamicAllocation.maxExecutors=100
spark.dynamicAllocation.initialExecutors=20
spark.dynamicAllocation.enabled: Activates dynamic allocation.
spark.dynamicAllocation.minExecutors: Sets the minimum number of executors.
spark.dynamicAllocation.maxExecutors: Defines the maximum number of executors.
spark.dynamicAllocation.initialExecutors: Specifies the number of executors to start with.
Executor Configuration

Optimizing executor settings involves balancing the number of executors, cores per executor, and memory allocation:

Cores per Executor: Allocating 5 cores per executor is a balanced approach, promoting parallelism while preventing excessive task contention. 
SPARK BY EXAMPLES

Executors per Node: With 160 cores per node (800 cores / 5 nodes), assigning 5 cores per executor results in 32 executors per node (160 cores / 5 cores per executor).

Memory per Executor: Distributing the node's memory among the executors, while reserving resources for system processes and overhead, is essential.

Available Memory per Node: 614.4 GB
Memory Allocation: Assuming 90% allocation to Spark: 614.4 GB * 0.9 ≈ 553 GB
Memory per Executor: 553 GB / 32 executors ≈ 17.28 GB
Considering overhead (approximately 7%):

Overhead per Executor: 17.28 GB * 0.07 ≈ 1.21 GB
Executor Memory Setting: 17.28 GB - 1.21 GB ≈ 16 GB
Spark Configuration Parameters

Set the following parameters to reflect the above calculations:

properties
Copy code
spark.executor.cores=5
spark.executor.memory=16g
spark.executor.memoryOverhead=1.2g
Data Partitioning

Efficient data partitioning enhances parallelism and performance. A common guideline is to have approximately 2–4 partitions per core. 
SPARK BY EXAMPLES

Total Cores: 800
Target Partitions: 800 cores * 3 partitions per core = 2,400 partitions
Adjust the number of partitions in your Spark job accordingly:

python
Copy code
rdd.repartition(2400)
Flowchart for Optimizing Spark Job Performance

To visualize the optimization process, consider the following flowchart:

mermaid
Copy code
graph TD
A[Start] --> B[Enable Dynamic Allocation]
B --> C[Configure Executor Settings]
C --> D[Set spark.executor.cores to 5]
D --> E[Calculate Memory per Executor]
E --> F[Set spark.executor.memory to 16g]
F --> G[Set spark.executor.memoryOverhead to 1.2g]
G --> H[Determine Optimal Number of Partitions]
H --> I[Set partitions to 2400]
I --> J[Run Spark Job]
J --> K[Monitor Performance]
K --> L{Performance Satisfactory?}
L --> |Yes| M[End]
L --> |No| N[Adjust Configurations and Re-run]
N --> J
Additional Optimization Tips

Data Serialization: Switch to Kryo serialization for improved performance. 
REINTECH

properties
Copy code
spark.serializer=org.apache.spark.serializer.KryoSerializer
Broadcast Joins: For small datasets, use broadcast joins to reduce shuffle operations.

python
Copy code
broadcasted_var = sc.broadcast(small_dataset)
Caching: Cache intermediate datasets that are reused to save computation time.

python
Copy code
df.cache()
By implementing these configurations and best practices, you can enhance the performance of your Spark job on the specified cluster, ensuring efficient processing of the 4 TB dataset.



Enhanced
Sources








You said:
If  you enable dynamic settting in maximum number of  executor and if you put custom number of executor which one will overwrite which
ChatGPT said:
ChatGPT
In Apache Spark, configuring executor allocation is crucial for optimizing application performance. When both dynamic allocation and static executor settings are specified, understanding their precedence is essential to ensure the desired behavior.

Dynamic Allocation Overview

Dynamic allocation allows Spark to adjust the number of executors during runtime based on the workload. This feature enables Spark to request additional executors when there are pending tasks and release idle ones, promoting efficient resource utilization. To enable dynamic allocation, set the following configurations:

properties
Copy code
spark.dynamicAllocation.enabled = true
spark.dynamicAllocation.minExecutors = [minimum number of executors]
spark.dynamicAllocation.maxExecutors = [maximum number of executors]
Additionally, ensure that the external shuffle service is enabled to allow executors to be removed without deleting shuffle files:

properties
Copy code
spark.shuffle.service.enabled = true
Static Executor Configuration

Static executor settings define a fixed number of executors for the entire duration of the Spark application. This is typically specified using the --num-executors parameter in the spark-submit command or by setting spark.executor.instances in the configuration:

bash
Copy code
spark-submit --num-executors [number of executors]
or

properties
Copy code
spark.executor.instances = [number of executors]
Precedence and Interaction

When both dynamic allocation and static executor settings are specified, their interaction is governed by specific precedence rules:

Dynamic Allocation Enabled with spark.executor.instances Set:

If spark.dynamicAllocation.enabled is set to true and spark.executor.instances is also specified, Spark will disable dynamic allocation. This is because specifying a fixed number of executors conflicts with the dynamic nature of executor allocation. A warning message will be logged indicating this configuration conflict. 
MALLIKARJUNA G
Dynamic Allocation Enabled without spark.executor.instances:

When dynamic allocation is enabled and spark.executor.instances is not set (or set to 0), Spark manages the number of executors based on the workload, within the bounds defined by spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors. In this scenario, dynamic allocation functions as intended, scaling resources up or down as needed. 
APACHE SPARK
Specifying --num-executors with Dynamic Allocation:

Using the --num-executors parameter in the spark-submit command sets the initial number of executors when dynamic allocation is enabled. However, it does not override the dynamic allocation settings. Spark will still adjust the number of executors based on the workload, but it will start with the number specified by --num-executors. It's important to note that if --num-executors is set higher than spark.dynamicAllocation.maxExecutors, the latter will take precedence, and Spark will not exceed the maximum specified by spark.dynamicAllocation.maxExecutors. 
CLOUDERA COMMUNITY
Best Practices

Avoid Conflicting Configurations: To prevent unintended behavior, avoid setting spark.executor.instances or using the --num-executors parameter when dynamic allocation is enabled. Let Spark manage the executor count within the specified dynamic allocation bounds.

Configure Dynamic Allocation Parameters Appropriately: Set spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors according to your application's workload characteristics to ensure optimal performance.

Enable External Shuffle Service: Ensure that the external shuffle service is enabled (spark.shuffle.service.enabled = true) when using dynamic allocation to allow for safe removal of idle executors without affecting shuffle data.

For a visual explanation of static versus dynamic resource allocation in Spark, you may find th





Data skew in Apache Spark occurs when data is unevenly distributed across partitions, leading to performance bottlenecks. Two primary strategies to address this issue are Salting and Adaptive Query Execution (AQE).

1. Salting: Manual Redistribution of Skewed Data

Salting involves adding a random value to skewed keys to distribute data more evenly across partitions. This manual technique requires identifying skewed keys and modifying them to balance the load.

How It Works:

Identify the Skewed Key: Determine which key(s) have a disproportionately large number of records.

Add Salt to the Key: Append a random value (salt) to the skewed key.

Join with Salted Keys: Match the salted keys in both datasets to redistribute the workload evenly.

2. Adaptive Query Execution (AQE): Automatic Skew Handling

AQE is a feature in Spark that automatically detects and optimizes skewed partitions during query execution. By enabling AQE, Spark can dynamically adjust execution plans to handle data skew without manual intervention.

How AQE Handles Skew:

Runtime Statistics Collection: Spark gathers data during the shuffle stage to identify skewed partitions.

Partition Adjustment: Based on the collected statistics, Spark splits large partitions into smaller sub-partitions or merges small ones to optimize performance.

Comparison: Salting vs. AQE

Feature	Salting	Adaptive Query Execution (AQE)
Effort Required	High (manual implementation)	Low (configuration only)
Applicability	Effective for known skewed keys	Handles both known and unknown skew
Execution Adjustment	Creates extra partitions to balance load	Dynamically adjusts partitions at runtime
Automation	No, manual identification and salting needed	Yes, automatic detection and optimization
When to Use Each Method:

Salting:

When the skew pattern is well-known and predictable.
For static data where skew is consistent over time.
AQE:

For dynamic workloads with unknown or unpredictable skew.
When runtime optimization is required without changing the code.
Mermaid Flowchart Code:

To visualize the process of handling data skew in Spark using both methods, here's a Mermaid flowchart:

mermaid
Copy code
flowchart TD
    A[Start: Detect Data Skew] --> B{Is Skew Pattern Known?}
    B -- Yes --> C[Apply Salting Technique]
    C --> D[Identify Skewed Keys]
    D --> E[Add Random Salt to Keys]
    E --> F[Redistribute Data Across Partitions]
    F --> G[Continue with Optimized Execution]
    B -- No --> H[Enable Adaptive Query Execution]
    H --> I[Set spark.sql.adaptive.enabled to true]
    I --> J[Automatic Detection and Optimization]
    J --> G[Continue with Optimized Execution]
This flowchart illustrates the decision-making process for handling data skew in Spark, guiding users to choose between manual salting and enabling AQE based on the nature of the data skew.





Data skew and memory spills are common challenges in Apache Spark that can significantly impact performance. Understanding these issues and implementing effective optimization strategies is crucial for efficient data processing. Below are five scenarios illustrating data skew, memory spills, and disk spills, along with optimization techniques and corresponding flowcharts for clarity.

1. Scenario: Skewed Data During Join Operations

Problem: When joining two datasets, if one dataset has a highly skewed key distribution, certain tasks may process disproportionately large amounts of data, leading to performance bottlenecks and potential memory spills.

Optimization Techniques:

Salting: Add a random value (salt) to the join keys of the skewed dataset to distribute the data more evenly across partitions.

Broadcast Joins: For smaller datasets, broadcast the smaller dataset to all nodes to avoid shuffling large amounts of data.

Flowchart:

mermaid
Copy code
graph TD
    A[Start Join Operation] --> B{Is Data Skewed?}
    B -- Yes --> C[Apply Salting to Skewed Keys]
    C --> D[Perform Join]
    B -- No --> D[Perform Join]
    D --> E[Check Dataset Size]
    E -- Small Dataset --> F[Use Broadcast Join]
    E -- Large Dataset --> G[Proceed with Standard Join]
    F --> H[Join Completed]
    G --> H[Join Completed]
2. Scenario: Memory Spill During Aggregations

Problem: Large aggregations can exceed available executor memory, causing Spark to spill data to disk, which degrades performance.

Optimization Techniques:

Increase Executor Memory: Allocate more memory to executors to handle large aggregations.

Optimize Aggregation Functions: Use efficient aggregation functions and consider combining operations to reduce memory usage.

Flowchart:

mermaid
Copy code
graph TD
    A[Start Aggregation] --> B[Monitor Memory Usage]
    B -- Exceeds Limit --> C[Increase Executor Memory]
    C --> D[Optimize Aggregation Functions]
    D --> E[Aggregation Completed]
    B -- Within Limit --> E[Aggregation Completed]
3. Scenario: Disk Spill During Shuffle Operations

Problem: During shuffle operations, if the data being shuffled exceeds the memory allocated for shuffle buffers, Spark spills the data to disk, leading to increased I/O operations and reduced performance.

Optimization Techniques:

Increase Shuffle Buffer Size: Adjust the spark.shuffle.file.buffer parameter to allocate more memory for shuffle operations.

Repartition Data: Increase the number of partitions to distribute data more evenly and reduce the amount of data per partition.

Flowchart:

mermaid
Copy code
graph TD
    A[Initiate Shuffle Operation] --> B[Monitor Shuffle Buffer Usage]
    B -- Buffer Full --> C[Increase Shuffle Buffer Size]
    C --> D[Repartition Data]
    D --> E[Shuffle Operation Completed]
    B -- Buffer Sufficient --> E[Shuffle Operation Completed]
4. Scenario: Skewed Data Causing Executor OOM (Out of Memory) Errors

Problem: Highly skewed data can cause certain executors to run out of memory, leading to task failures and job retries.

Optimization Techniques:

Data Sampling and Analysis: Analyze the data distribution to identify skewed keys.

Custom Partitioning: Implement a custom partitioner to distribute data more evenly across executors.

Flowchart:

mermaid
Copy code
graph TD
    A[Analyze Data Distribution] --> B{Skewed Keys Detected?}
    B -- Yes --> C[Implement Custom Partitioner]
    C --> D[Distribute Data Evenly]
    D --> E[Execute Tasks]
    B -- No --> E[Execute Tasks]
    E --> F[Monitor Executor Memory]
    F -- OOM Errors --> G[Reassess Data Distribution]
    F -- No Errors --> H[Job Completed Successfully]
5. Scenario: Inefficient Joins Leading to Disk Spills

Problem: Performing joins without considering data distribution can lead to large shuffle operations, causing disk spills and degraded performance.

Optimization Techniques:

Broadcast Smaller Tables: Use broadcast joins for smaller tables to eliminate the need for shuffling large datasets.

Adjust Join Strategies: Configure Spark to use the most efficient join strategy based on data sizes and distribution.

Flowchart:

mermaid
Copy code
graph TD
    A[Determine Join Operation] --> B[Assess Table Sizes]
    B -- Small Table Detected --> C[Apply Broadcast Join]
    C --> D[Execute Join]
    B -- Large Tables --> E[Analyze Data Distribution]
    E -- Skewed Data --> F[Implement Skew Handling Techniques]
    F --> D[Execute Join]
    E -- Even Distribution --> D[Execute Join]
    D --> G[Monitor for Disk Spills]
    G -- Spills Detected --> H[Optimize Join Strategy]
    G -- No Spills --> I[Join Completed Successfully]
Implementing these optimization techniques can significantly enhance Spark job performance by addressing data skew and preventing memory and disk spills. Regular monitoring and analysis of data distribution, along with appropriate configuration adjustments, are key to maintaining efficient Spark applications.

For more detailed information on Spark optimization techniques, refer to the Apache Spark Performance Tuning Guide.



Enhanced
Sources

[all:vars]
# Versions
jsvc_version=1.4.0
java_version=11
hadoop_version=3.4.0
zookeeper_version=3.9.2
spark_version=3.5.3
airflow_version=2.9.3
python_version=3.9.20
# Service Names
service_group_name=hadoop
airflow_service_name=airflow
hadoop_hdfs_service_name=hdfs
hadoop_yarn_service_name=yarn
hadoop_mapred_service_name=mapred
postgresql_service_name=postgres
spark_service_name=spark
zookeeper_service_name=zookeeper
# Kerberos Settings
krb5kdc_admin_password="Br!ngM3@llTh3M0n3y"
# Zookeeper Settings
zookeeper_data_dir=/data/zookeeper
# Hadoop Global Settings
hadoop_keystore_password="Br!ngM3@llTh3M0n3y"
hadoop_java_home=/usr/lib/jvm/jre-{{ java_version }}-openjdk/
hadoop_nameservices=hadoopDev
hadoop_lang=en_US.UTF-8
hadoop_heapsize_max=500m
hadoop_heapsize_min=25m
hadoop_niceness=15
# Hadoop HDFS Settings
hadoop_dfs_replication=3
hadoop_dfs_namenode_name_dir=/data/namenode
hadoop_dfs_journalnode_edits_dir=/data/journalnode
hadoop_dfs_datanode_data_dir=/data/datanode
# Hadoop YARN Settings
hadoop_yarn_resourcemanager_dir=/data/resourcemanager
hadoop_yarn_nodemanager_dir=/data/nodemanager
hadoop_yarn_timelineserver_dir=/data/timelineserver
# Apache Airflow Settings
#airflow_python_packages="celery,kerberos,apache-spark,apache-hdfs,postgres"
airflow_python_packages="celery,apache-spark,postgres"
airflow_master_dir=/data/airflow
airflow_venv=/opt/airflow

[gluster_master_nodes]
zmn01 ansible_host=ch03-colbt-dv-zmn01.od.corpdev.transunion.com gluster_master=yes
zmn02 ansible_host=ch03-colbt-dv-zmn02.od.corpdev.transunion.com
zmn03 ansible_host=ch03-colbt-dv-zmn03.od.corpdev.transunion.com

[kerberos_master_nodes]
zmn01 ansible_host=ch03-colbt-dv-zmn01.od.corpdev.transunion.com krb5kdc_master=yes
zmn02 ansible_host=ch03-colbt-dv-zmn02.od.corpdev.transunion.com
zmn03 ansible_host=ch03-colbt-dv-zmn03.od.corpdev.transunion.com

[zookeeper_master_nodes]
zmn01 ansible_host=ch03-colbt-dv-zmn01.od.corpdev.transunion.com zookeeper_id=1 zookeeper_master=yes
zmn02 ansible_host=ch03-colbt-dv-zmn02.od.corpdev.transunion.com zookeeper_id=2
zmn03 ansible_host=ch03-colbt-dv-zmn03.od.corpdev.transunion.com zookeeper_id=3

[hadoop_hdfs_master_nodes]
hmn01 ansible_host=ch03-colbt-dv-hmns01.od.corpdev.transunion.com hadoop_hdfs_master=yes
hmn02 ansible_host=ch03-colbt-dv-hmns02.od.corpdev.transunion.com
hmn03 ansible_host=ch03-colbt-dv-hmns03.od.corpdev.transunion.com hadoop_hdfs_observer=yes

[hadoop_hdfs_journal_nodes]
zmn01 ansible_host=ch03-colbt-dv-zmn01.od.corpdev.transunion.com
zmn02 ansible_host=ch03-colbt-dv-zmn02.od.corpdev.transunion.com
zmn03 ansible_host=ch03-colbt-dv-zmn03.od.corpdev.transunion.com

[hadoop_yarn_master_nodes]
hmn01 ansible_host=ch03-colbt-dv-hmns01.od.corpdev.transunion.com hadoop_yarn_master=yes
hmn02 ansible_host=ch03-colbt-dv-hmns02.od.corpdev.transunion.com
hmn03 ansible_host=ch03-colbt-dv-hmns03.od.corpdev.transunion.com

[hadoop_spark_master_nodes]
hmn01 ansible_host=ch03-colbt-dv-hmns01.od.corpdev.transunion.com
hmn02 ansible_host=ch03-colbt-dv-hmns02.od.corpdev.transunion.com
hmn03 ansible_host=ch03-colbt-dv-hmns03.od.corpdev.transunion.com

[airflow_master_nodes]
#amn01 ansible_host=ch03-colbt-dv-amn01.od.corpdev.transunion.com
#amn02 ansible_host=ch03-colbt-dv-amn02.od.corpdev.transunion.com
amn03 ansible_host=ch03-colbt-dv-amn03.od.corpdev.transunion.com

[postgresql_master_nodes]
pmn01 ansible_host=ch03-colbt-dv-pmn01.od.corpdev.transunion.com
pmn02 ansible_host=ch03-colbt-dv-pmn02.od.corpdev.transunion.com

[hadoop_hdfs_data_nodes]
hdn01 ansible_host=ch03-colbt-dv-hdn01.od.corpdev.transunion.com
hdn02 ansible_host=ch03-colbt-dv-hdn02.od.corpdev.transunion.com
hdn03 ansible_host=ch03-colbt-dv-hdn03.od.corpdev.transunion.com
hdn04 ansible_host=ch03-colbt-dv-hdn04.od.corpdev.transunion.com
hdn05 ansible_host=ch03-colbt-dv-hdn05.od.corpdev.transunion.com
hdn06 ansible_host=ch03-colbt-dv-hdn06.od.corpdev.transunion.com

[hadoop_yarn_compute_nodes]
hcn01 ansible_host=ch03-colbt-dv-hcn01.od.corpdev.transunion.com
hcn02 ansible_host=ch03-colbt-dv-hcn02.od.corpdev.transunion.com
hcn03 ansible_host=ch03-colbt-dv-hcn03.od.corpdev.transunion.com
hcn04 ansible_host=ch03-colbt-dv-hcn04.od.corpdev.transunion.com
hcn05 ansible_host=ch03-colbt-dv-hcn05.od.corpdev.transunion.com
hcn06 ansible_host=ch03-colbt-dv-hcn06.od.corpdev.transunion.com

[tomcat_apache]
tomapp1 ansible_host=ch03-colbt-dv-testssh534.od.corpdev.transunion.com
tomcat_lab_col ansible_host=ch03colondvlabtom01.od.corpdev.transunion.com
tomcat_dev_col ansible_host=ch03-colbt-dv-tomcat06.od.corpdev.transunion.com

[apache]
apache1 ansible_host=10.199.100.50
apache_dev_col ansible_host=10.199.100.18
