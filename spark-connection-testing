Create a test DAG using SparkSubmitOperator:

python
Copy code
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime

dag = DAG(
    'test_spark_connection',
    schedule_interval=None,
    start_date=datetime(2023, 1, 1),
)

spark_test = SparkSubmitOperator(
    task_id='test_spark_submit',
    application='local:///path/to/test_spark_job.py',
    conn_id='spark_default',
    verbose=True,
    dag=dag,
)
Replace /path/to/test_spark_job.py with the path to a simple Spark job file, such as:

python
Copy code
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TestJob").getOrCreate()
df = spark.createDataFrame([(1, 'test')], ['id', 'value'])
df.show()
spark.stop()
Trigger the DAG and check the logs to ensure the connection is working:

bash
Copy code
airflow dags trigger test_spark_connection
