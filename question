As discussed, we were unable to find any specific error messages or concrete evidence in the HDFS logs related to this incident. We observed that the cluster load was slightly elevated at the time, but it was still within normal operating limits, and the NameNode should be capable of handling that level of load.

We also reviewed the DG-UPLOAD-Service log files and did not find any clear error messages corresponding to the reported issue.

However, we identified that there are two application servers involved, and one of them is currently not reporting metrics to Datadog. Because of this, we are unsure whether the upload attempt may have originated from that server.

I have a few follow-up questions to help narrow this down. Could you please review and provide clarification?


üìå DG-UPLOAD ‚Üí HDFS Investigation Questions

1Ô∏è‚É£ Can you share the full DG-UPLOAD-Service stack trace with the exact timestamp of the failure?

2Ô∏è‚É£ When did this issue start, and how frequently is it occurring?

3Ô∏è‚É£ At the time of failure, were you able to access https://hdfs
? Did it load normally, slowly, or not at all?

4Ô∏è‚É£ Does the issue happen randomly, during peak hours, or on every upload?

5Ô∏è‚É£ If it happens again, can you capture:

Screenshot of the DG-UPLOAD error log

Screenshot of the HDFS page

Exact time of occurrence

These details will help us determine whether this is an application, network, or HDFS service issue.


At the time uploads were successful, did you observe any Kafka connectivity issues, or was Kafka functioning normally?
